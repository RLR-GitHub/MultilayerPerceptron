# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CvzQo-pkEuQQKjWjcs2lpy0-GKyE4Pha

# Library & Variable Declarations
"""

import math
import numpy as np
import matplotlib.pyplot as plt
from random import random, seed, uniform

N = 2               # Number of inputs (x, y)
J = 8               # Number of hidden neurons
K = 1               # Number of output neurons
I = N+1             # Number of inputs with bias included
M = 225             # Number of training patterns (I/O pairs)
MTEST = 225         # Number of test set inputs
EPOCHS = 500        # Complete training set pass-throughs
LEARNING_RATE = 0.5 # learning rate (rho )

"""# Setup Matrices"""

X_train = np.zeros( shape = ( M, I ) )      # Training set inputs
D_train = np.zeros( shape = ( M, K ) )      # Training set target vector
Y_train = np.zeros( shape = ( M, K ) )      # Training set output vector

X_test  = np.zeros( shape = ( MTEST, I ) )  # Test set inputs
D_test  = np.zeros( shape = ( MTEST, K ) )  # Test set target vector
Y_test  = np.zeros( shape = ( MTEST, K ) )  # Test set output vector
#===================================================================================
x = np.zeros( shape = ( I ) )               # Single input vector from training set
d = np.zeros( shape = ( K ) )               # Single target vector from training set

h = np.zeros( shape = ( J ) )               # Weighted sum for hidden layer ( Z )
z = np.zeros( shape = ( J ) )               # Vector of hidden layer neurons

o = np.zeros( shape = ( K ) )               # Weiughted sum for output layer ( Y )
y = np.zeros( shape = ( K ) )               # Single vector of outupt neurons

delta = np.zeros( shape = ( K ) )           # Value to gauge error gradient
#===================================================================================
V = np.zeros( shape = ( J, I ) )            # Weight matrix for hidden layer 1
W = np.zeros( shape = ( K, J ) )            # Weight matrix for hidden layer 2

delta_V = np.zeros( shape = ( J, I ) )      # Weight changes for hidden layer 1
delta_W = np.zeros( shape = ( K, J ) )      # Weight changes for hidden layer 2

"""# FUNCTIONS"""

def randomMatrix( matrix, min_val, max_val, bias = False ):
    r, c = matrix.shape
    random_matrix = np.random.uniform( low = min_val, high = max_val, size = ( r, c ) )
    if( bias == True ):
        for i in range( r ): random_matrix[ i ][ c - 1 ] = 1
    return( random_matrix )

def dotProduct( matrix, vector ):
    dot_matrix = np.dot( matrix, vector )
    return( dot_matrix )

def trainingInputs( matrix ): ### MAY BE WRONG
    training_matrix = randomMatrix( matrix, -1.0, 1.0, bias = True )
    return( training_matrix )

def testingInputs( matrix ):
    inc = 0
    r, c = matrix.shape
    numSides = math.sqrt( r )
    step = 2 / ( numSides - 1 )

    for i in range( r ):
        if( ( i % numSides == 0 ) and ( i != 0 ) ): inc = inc + 1
        matrix[ i ][ 1 ] = 1.0 - step * inc               # values from 1 to -1
        matrix[ i ][ 0 ] = -1.0 + step * ( i % numSides ) # values from -1 to 1
        matrix[ i ][ 2 ] = 1
    return( matrix )

def targetOutputs( targets, inputs, area ):
    radius = math.sqrt( area / math.pi )
    r, c = targets.shape

    for i in range( r ):
        radii = math.sqrt( ( inputs[ i ][ 0 ] * inputs[ i ][ 0 ] ) + ( inputs[ i ][ 1 ] * inputs[ i ][ 1 ] ) )
        if( radii <= radius ): targets[ i ][ 0 ] = 1
        else: targets[ i ][ 0 ] = 0

    return( targets )

def sigmoidActivation( net ):
    n = net.size
    sig_vec = np.zeros( shape = ( n ) )

    for i in range( n ):
        sig_vec[ i ] = 1 / ( 1 + math.exp( -1 * net[ i ] ) )

    return( sig_vec )


def weightChanges_W( delta, z, lr = LEARNING_RATE ):
    delta_W = np.zeros( shape = ( K, J ) )

    for j in range( J ):
        delta_W[ 0 ][ j ] = lr * delta * z[ j ]

    return( delta_W )

def weightChanges_V( W, delta, z, x, lr = LEARNING_RATE ):
    delta_V = np.zeros( shape = ( J, I ) )

    for j in range( J ):
        summation = delta * W[ 0 ][ j ]
        for i in range( I ):
            delta_V[ j ][ i ] = lr * z[ j ] * ( 1 - z[ j ] ) * x[ i ] * summation

    return( delta_V )

def updateWeights( delta_matrix, matrix ):
    r, c = matrix.shape

    for i in range( r ):
        for j in range( c ):
            matrix[ i ][ j ] = delta_matrix[ i ][ j ] + matrix[ i ][ j ]

    return( matrix )

def printMatrix( matrix, mat2, string ):
    size = matrix.size
    fig, ax = plt.subplots( figsize = ( 5,5 ) )
    ax.set_ylim( [ -1, 1 ] )
    ax.set_xlim( [ -1, 1 ] )
    ax.set_title( string, va = 'bottom' )
    for r in range( size ):
        val = int( round( matrix[ r ][ 0 ] ) )
        if( val == 1 ): ax.text( mat2[ r ][0], mat2[r][1], str( val ), va = 'center', ha = 'center', c = 'r' )
        if( val == 0 ): ax.text( mat2[ r ][0], mat2[r][1], str( val ), va = 'center', ha = 'center', c = 'b' )

"""# TRAINING PHASE"""

# GET TRAINING PATTERN VALUES
X_train = trainingInputs( X_train ) # random points in [-1,1]x[-1,1]
D_train = targetOutputs( D_train, X_train, area = 2.0 )

# INITIALIZE WEIGHT MATRICIES
V = randomMatrix( V, -10.0, 10.0) # Weight matrix to the hidden layer
W = randomMatrix( W, -10.0, 10.0) # Weight matrix to the output layer

new_lr = 0.5#0.75#LEARNING_RATE

for epoch in range( EPOCHS ):

    Error = 0.0
    for m in range( M ):

        # FORWARD PASS

        x = X_train[ m ][ : ]                               # Get mth row of X
        d = D_train[ m ][ : ]                               # Get mth row of D

        h = dotProduct( V, x )                              # Get weighted sums of hidden layer
        z = sigmoidActivation( h )                          # Get weighted outputs of hidden layer

        o = dotProduct( W, z )                              # Get weighted sums of output layer
        y = sigmoidActivation( o )

        Y_train[ m ] = y

        # BACKWARD PASS

        E = ( d - y )                                       # Gradient of error
        delta = E * y * ( 1 - y )                           # Chnage factor (delta) at output layer
        Error = Error + ( E * E ) / 2                       # Actual error: mean square loss

        delta_W = weightChanges_W( delta, z, new_lr )       # Compute weight changes of W
        delta_V = weightChanges_V( W, delta, z, x, new_lr ) # Compute weight changes of V

        W = updateWeights( delta_W, W )                     # Compute weight update of W
        V = updateWeights( delta_V, V )                     # Compute weight update of V

    if( ( epoch + 1 ) % 20 == True ): print( "Epoch: {0:4d}\t".format( int( epoch ) ), "Error: {0:8}\t".format( float( Error ) ), "LR: {0:5f}".format( float( new_lr ) ) )

"""# TESTING PHASE"""

# GET TESTING PATTERN VALUES
X_test = testingInputs( X_test )
D_test = targetOutputs( D_test, X_test, area = 2.0 )

Error = 0.0
for m in range( MTEST ):

    x = X_test[ m ][ : ]                       # Get mth row of X
    d = D_test[ m ][ : ]                       # Get mth row of D

    h = dotProduct( V, x )                     # Get weighted sums of hidden layer
    z = sigmoidActivation( h )                 # Get weighted outputs of hidden layer

    o = dotProduct( W, z )                     # Get weighted sums of output layer
    y = sigmoidActivation( o )                 # Get weighted outputs of output layer

    Error = Error + ( d - y ) * ( d - y ) / 2  # Actual error: mean square loss
    Y_test[ m ] = y

print( "TestSet Error:", float( Error ) )

"""# RESULTS"""

printMatrix( Y_train, X_train, "D_TRAIN TRAINING_TARGETS" )
printMatrix( D_train, X_train, "D_TRAIN TRAINING_TARGETS" )
printMatrix(  Y_test,  X_test, "Y_TEST TESTING_OUTPUTS" )
printMatrix(  D_test,  X_test, "D_TEST TESTING_TARGETS" )

"""# DEBUG"""

print( h )
print( "\nZ",z )
print( "\nO",o )
print( "\nY",y )
print( "\nD",d )

print( "\ndelta", delta )
print( "\nE",E )

print( V )
print( "\n",W )
